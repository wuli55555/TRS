from models.i3d import InceptionI3d
from models.evaluator import *
import torch
import torch.nn as nn
 

class TRS(nn.Module):
    def __init__(self, feature_dim):
        super(TRS, self).__init__()
        self.feature_dim = feature_dim
        self.i3d = InceptionI3d()
        self.evaluator = MLPMixer(depth=8, num_classes=2, token_dim=512, channel_dim= 2048)
        self.evaluator_r = Evaluator_relative(output_dim=2)

    def forward(self, x, xr=None, use_relative=True, predict=False):
        x.transpose_(1, 2)  # N, C, T, H, W
        batch_size, C, frames, H, W = x.shape
        clip_feats = torch.empty(batch_size, 10, self.feature_dim).cuda()
        for i in range(9):
            clip_feats[:, i] = self.i3d(x[:, :, 10 * i:10 * i + 16, :, :]).squeeze(2)
        clip_feats[:, 9] = self.i3d(x[:, :, -16:, :, :]).squeeze(2)

        probs = self.evaluator(clip_feats)

        if use_relative:
            xr.transpose_(1, 2)
            clip_feats_c = torch.empty(batch_size, 10, self.feature_dim).cuda()
            for i in range(9):
                clip_feats_c[:, i] = self.i3d(xr[:, :, 10 * i:10 * i + 16, :, :]).squeeze(2)
            clip_feats_c[:, 9] = self.i3d(xr[:, :, -16:, :, :]).squeeze(2)

            relative_probs = self.evaluator_r(clip_feats,clip_feats_c)

            if  predict:
                return probs, relative_probs
            else:
                relative_probs2 = self.evaluator_r(clip_feats_c,clip_feats)
                probs_c = self.evaluator(clip_feats_c)
                return probs, probs_c, relative_probs, relative_probs2
        else:
            return probs
